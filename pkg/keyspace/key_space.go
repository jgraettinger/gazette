// Package keyspace implements an efficient mechanism to mirror a decoded Etcd
// key space into a local KeySpace, which may be kept updated via a long-lived
// Watch operation. KeySpace scales efficiently to 10's and even 100's of
// thousands of keys, while providing fast range and point queries powered by
// its packed, sorted ordering.
package keyspace

import (
	"bytes"
	"container/heap"
	"context"
	"fmt"
	"path"
	"sort"
	"sync"
	"time"

	"github.com/coreos/etcd/clientv3"
	"github.com/coreos/etcd/clientv3/mirror"
	"github.com/coreos/etcd/etcdserver/etcdserverpb"
	log "github.com/sirupsen/logrus"
)

// A KeySpace is a local mirror of a decoded portion of the Etcd key/value space,
// which is kept in sync with Etcd via long-lived Watch operations. KeySpace
// must be read-locked before access, to guard against concurrent updates.
type KeySpace struct {
	// Key prefix which roots this KeySpace.
	Root string
	// Header is the last Etcd operation header which updated this KeySpace.
	Header etcdserverpb.ResponseHeader
	// KeyValues is a complete and decoded mirror of the (prefixed) Etcd key/value space.
	KeyValues
	// Mu guards Header and KeyValues. It must be read-locked before either is accessed.
	Mu sync.RWMutex

	decode KeyValueDecoder
	next   KeyValues
}

// NewKeySpace returns a KeySpace with the configured key |prefix| and |decoder|.
// |prefix| must be a "Clean" path, as defined by path.Clean, or NewKeySpace panics.
// This check limits the space of possible prefixes somewhat, but guards against
// many common and unintentional errors (eg, mixed use of trailing slashes).
func NewKeySpace(prefix string, decoder KeyValueDecoder) *KeySpace {
	if path.Clean(prefix) != prefix {
		panic("expected prefix to be a cleaned path")
	}
	return &KeySpace{Root: prefix, decode: decoder}
}

// Load loads a snapshot of the prefixed KeySpace at revision |rev|,
// or if |rev| is zero, at the current revision.
func (ks *KeySpace) Load(ctx context.Context, client *clientv3.Client, rev int64) error {
	defer ks.Mu.Unlock()
	ks.Mu.Lock()

	ks.Header, ks.KeyValues = etcdserverpb.ResponseHeader{}, ks.KeyValues[:0]
	var respCh, errCh = mirror.NewSyncer(client, ks.Root, rev).SyncBase(ctx)

	// Read messages across |respCh| and |errCh| until both are closed.
	for respCh != nil || errCh != nil {
		select {
		case resp, ok := <-respCh:
			if !ok {
				respCh = nil // Finished draining |respCh|.
			} else if err := patchHeader(&ks.Header, *resp.Header, true); err != nil {
				return err
			} else {
				for _, kv := range resp.Kvs {
					if ks.KeyValues, err = appendKeyValue(ks.KeyValues, ks.decode, kv); err != nil {
						log.WithFields(log.Fields{"key": string(kv.Key), "err": err}).
							Error("inconsistent key/value while loading")
					}
				}
			}
		case err, ok := <-errCh:
			if !ok {
				errCh = nil // Finished draining |errCh|.
			} else {
				return err
			}
		}
	}
	return nil
}

// Watch a loaded KeySpace and apply updates as they are received. If
// non-nil, Watch will deliver signals to |signalCh| such that |signalCh| is
// selectable iff one or more updates have been applied since the last signal
// delivery. |signalCh| is closed when Watch exits, and it's recommended but
// not required that |signalCh| be unbuffered.
func (ks *KeySpace) Watch(ctx context.Context, client clientv3.Watcher, signalCh chan<- struct{}) error {
	var watchCh clientv3.WatchChan

	// Note that nil channels never select. Leverage this to toggle |signalCh| below.
	var maybeSignalCh chan<- struct{}
	if signalCh != nil {
		defer close(signalCh)
	}

	ks.Mu.RLock()
	// Begin a new long-lived, auto-retried Watch. Note this is very similar to
	// mirror.Syncer: A key difference (and the reason that API is not used) is
	// the additional WithProgressNotify option - without this option, in the
	// event of a long-lived watch over a prefix which has no key changes, Etcd
	// may compact away the watch revision and a retried Watch will later fail.
	// WithProgressNotify ensures the watched revision is kept reasonably recent
	// even if no WatchResponses otherwise arrive.
	watchCh = client.Watch(ctx, ks.Root,
		clientv3.WithPrefix(),
		clientv3.WithProgressNotify(),
		clientv3.WithRev(ks.Header.Revision+1),
	)
	ks.Mu.RUnlock()

	// WatchResponses can often arrive in quick succession and contain many key
	// events. We amortize the cost of updating the KeySpace by briefly delaying
	// the application of a first WatchResponse to await further events, and then
	// applying all events as a single bulk update.
	var responses []clientv3.WatchResponse
	var applyTimer = time.NewTimer(time.Hour) // Not possible to create an idle Timer.

	for {
		select {
		case resp, ok := <-watchCh:
			if !ok {
				return ctx.Err()
			} else if err := resp.Err(); err != nil {
				return err
			} else if len(responses) == 0 {
				applyTimer.Reset(watchApplyDelay)
			}
			responses = append(responses, resp)
		case <-applyTimer.C:
			if err := ks.apply(responses); err != nil {
				return err
			}
			responses = responses[:0]
			maybeSignalCh = signalCh
		case maybeSignalCh <- struct{}{}:
			maybeSignalCh = nil
		}
	}
}

// apply applies Etcd WatchResponses to the KeySpace. It returns only
// unrecoverable Watch errors; inconsistencies in the updates themselves
// are logged.
func (ks *KeySpace) apply(responses []clientv3.WatchResponse) error {
	for _, r := range responses {
		if err := patchHeader(&ks.Header, r.Header, false); err != nil {
			return err
		}
		// Order events of each WatchResponse on key order (they already share ModRevision).
		sort.Slice(r.Events, func(i, j int) bool {
			return bytes.Compare(r.Events[i].Kv.Key, r.Events[j].Kv.Key) < 0
		})
	}
	// Heap WatchResponses on (Key, ModRevision) order of the first response Event.
	var responseHeap = watchResponseHeap(responses)
	heap.Init(&responseHeap)

	// We'll build a new KeyValues into |next| via a single iteration over the
	// key space. Unmodified runs of keys are copied from |current|, with
	// Watch Events applied as they are encountered and in (Key, ModRevision) order.
	var current, next = ks.KeyValues, ks.next

	for responseHeap.Len() != 0 {
		// wr.Events[0] is the next Event in the overall sequence ordered on (Key, ModRevision).
		var wr = heap.Pop(&responseHeap).(clientv3.WatchResponse)

		// Find the |ind| of this key in |current|. Copy and append all preceding
		// keys to |next|, through a current value for this key. Then step |current|
		// forward, marking this range as completed.
		var ind, found = current.Search(string(wr.Events[0].Kv.Key))
		if found {
			ind++
		}
		next, current = append(next, current[:ind]...), current[ind:]

		// Patch the tail of |next|, inserting, modifying, or deleting at the last element.
		var err error
		if next, err = updateKeyValuesTail(next, ks.decode, *wr.Events[0]); err != nil {
			log.WithFields(log.Fields{"err": err, "event": wr.Events[0].Kv.String()}).
				Error("inconsistent watched key/value event")
		}
		// Pop wr.Events[0], and re-queue |wr| if events remain in this WatchResponse.
		if wr.Events = wr.Events[1:]; len(wr.Events) != 0 {
			heap.Push(&responseHeap, wr)
		}
	}
	next = append(next, current...) // Append any left-over elements in |current|.

	ks.Mu.Lock()
	ks.KeyValues, ks.next = next, ks.KeyValues[:0]
	ks.Mu.Unlock()

	return nil
}

// patchHeader updates |h| with an Etcd ResponseHeader. It returns an error if the headers are inconsistent.
// If |expectSameRevision|, both ResponseHeaders should reference the same Revision (or, |h| must be zero-valued).
// Otherwise, the |update| Revision is expected to be greater than the current one.
func patchHeader(h *etcdserverpb.ResponseHeader, update etcdserverpb.ResponseHeader, expectSameRevision bool) error {
	if h.ClusterId != 0 && h.ClusterId != update.ClusterId {
		return fmt.Errorf("etcd ClusterID mismatch (expected %d, got %d)", h.ClusterId, update.ClusterId)
	} else if expectSameRevision && h.Revision != update.Revision && *h != (etcdserverpb.ResponseHeader{}) {
		return fmt.Errorf("etcd Revision mismatch (expected = %d, got %d)", h.Revision, update.Revision)
	} else if !expectSameRevision && h.Revision >= update.Revision {
		return fmt.Errorf("etcd Revision mismatch (expected > %d, got %d)", h.Revision, update.Revision)
	}

	if h.MemberId != update.MemberId || h.RaftTerm != update.RaftTerm {
		log.WithFields(log.Fields{
			"memberId":        h.MemberId,
			"raftTerm":        h.RaftTerm,
			"update.MemberId": update.MemberId,
			"update.RaftTerm": update.RaftTerm,
		}).Info("etcd MemberId/RaftTerm changed")
	}

	*h = update
	return nil
}

// watchResponseHeap implements heap.Interface over WatchResponses.
type watchResponseHeap []clientv3.WatchResponse

func (h watchResponseHeap) Len() int            { return len(h) }
func (h watchResponseHeap) Swap(i, j int)       { h[i], h[j] = h[j], h[i] }
func (h *watchResponseHeap) Push(x interface{}) { *h = append(*h, x.(clientv3.WatchResponse)) }

// Less orders on ascending key, and for the same key, ascending ModRevision.
func (h watchResponseHeap) Less(i, j int) bool {
	if c := bytes.Compare(h[i].Events[0].Kv.Key, h[j].Events[0].Kv.Key); c != 0 {
		return c < 0
	}
	return h[i].Events[0].Kv.ModRevision < h[j].Events[0].Kv.ModRevision
}

func (h *watchResponseHeap) Pop() interface{} {
	var old, n = *h, len(*h)
	var x = old[n-1]
	*h = old[0 : n-1]
	return x
}

var watchApplyDelay = time.Millisecond * 20
